{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/17 05:18:12 WARN Utils: Your hostname, irfandi resolves to a loopback address: 127.0.1.1; using 192.168.18.211 instead (on interface ens160)\n",
      "22/10/17 05:18:12 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/ddi/spark-3.3.0-bin-hadoop3/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ddi/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ddi/.ivy2/jars\n",
      "neo4j-contrib#neo4j-connector-apache-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-31f2735c-eba7-470d-8d8d-704702e85e8a;1.0\n",
      "\tconfs: [default]\n",
      "\tfound neo4j-contrib#neo4j-connector-apache-spark_2.12;4.0.1_for_spark_3 in spark-packages\n",
      ":: resolution report :: resolve 218ms :: artifacts dl 9ms\n",
      "\t:: modules in use:\n",
      "\tneo4j-contrib#neo4j-connector-apache-spark_2.12;4.0.1_for_spark_3 from spark-packages in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-31f2735c-eba7-470d-8d8d-704702e85e8a\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 1 already retrieved (0kB/6ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/17 05:18:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/17 05:18:16 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "\t.config('spark.jars.packages', 'neo4j-contrib:neo4j-connector-apache-spark_2.12:4.0.1_for_spark_3')\\\n",
    "\t.master(\"local\").appName(\"spark_test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD NODES "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+--------------------+--------+---------+\n",
      "|        cif|     name|             address|    city|     type|\n",
      "+-----------+---------+--------------------+--------+---------+\n",
      "|18346875234|     Layo|89012 Autumn Leaf...| Jakarta|WHOLESALE|\n",
      "|18752032720| Topdrive|       83 Chive Pass|Surabaya|WHOLESALE|\n",
      "|18806746420|    Meetz|9411 Schmedeman C...| Bandung|WHOLESALE|\n",
      "|20476940876|     Vipe|4143 Daystar Terrace|  Bekasi|WHOLESALE|\n",
      "|19168353628|Buzzshare|       6 Ohio Avenue|   Medan|WHOLESALE|\n",
      "+-----------+---------+--------------------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "DataFrame[cif: string, name: string, address: string, city: string, type: string]\n"
     ]
    }
   ],
   "source": [
    "# show data \n",
    "df = spark.read.option(\"delimiter\", \";\").option(\"header\", \"true\").csv(\"/home/ddi/spark-3.3.0-bin-hadoop3/data/bankingdata/cif.csv\")\n",
    "df.show(5)\n",
    "print (df)\n",
    "\n",
    "\n",
    "# df3.select(\"fullname\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/17 05:36:19 WARN SchemaService: Switching to query schema resolution\n"
     ]
    }
   ],
   "source": [
    "# LOAD DATA \n",
    "df.write \\\n",
    "  .format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .option(\"url\", \"bolt://192.168.18.211:7687\") \\\n",
    "  .option(\"database\", \"irfandi\") \\\n",
    "  .mode(\"append\") \\\n",
    "  .option(\"authentication.basic.username\", \"neo4j\") \\\n",
    "  .option(\"authentication.basic.password\", \"ddi123\") \\\n",
    "  .option(\"labels\", \":Customer\") \\\n",
    "  .option(\"node.keys\", \"cif\") \\\n",
    "  .save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACCOUNT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----------+\n",
      "|        cif| account_no| open_date|\n",
      "+-----------+-----------+----------+\n",
      "|18346875234|  442222452|26-07-2017|\n",
      "|18752032720|13335322450|21-12-2021|\n",
      "|18752032720|13335322472|26-06-2013|\n",
      "|18806746420| 2237072704|19-03-2021|\n",
      "|18806746420|12015201770|16-07-2018|\n",
      "+-----------+-----------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "DataFrame[cif: string, account_no: string, open_date: string]\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.read.option(\"delimiter\", \";\").option(\"header\", \"true\").csv(\"/home/ddi/spark-3.3.0-bin-hadoop3/data/bankingdata/account.csv\")\n",
    "df1.show(5)\n",
    "print (df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/17 05:36:39 WARN SchemaService: Switching to query schema resolution\n"
     ]
    }
   ],
   "source": [
    "df1.write \\\n",
    "  .format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .option(\"url\", \"bolt://192.168.18.211:7687\") \\\n",
    "  .option(\"database\", \"irfandi\") \\\n",
    "  .mode(\"append\") \\\n",
    "  .option(\"authentication.basic.username\", \"neo4j\") \\\n",
    "  .option(\"authentication.basic.password\", \"ddi123\") \\\n",
    "  .option(\"labels\", \":Account\") \\\n",
    "  .option(\"node.keys\", \"account_no\") \\\n",
    "  .save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRANSACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------------+-----------+----------+-----------+\n",
      "|trx_id|  trx_date|account_from|     amount|account_to|trx_channel|\n",
      "+------+----------+------------+-----------+----------+-----------+\n",
      "|  3482|22/06/2021|   599280088| 25031037.0|1755637562|        SMS|\n",
      "|  4463|26/04/2021|    99677304|135827850.0|2266089634|        SMS|\n",
      "|  9002|17/05/2021|    99677304| 45697816.0| 186714864|        SMS|\n",
      "| 13021|26/08/2021|    18286974| 13895927.0| 349079568|        SMS|\n",
      "| 13118|23/08/2021|   411650844|   106963.0| 458800460|        SMS|\n",
      "+------+----------+------------+-----------+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "DataFrame[trx_id: string, trx_date: string, account_from: string, amount: string, account_to: string, trx_channel: string]\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.read.option(\"delimiter\", \";\").option(\"header\", \"true\").csv(\"/home/ddi/spark-3.3.0-bin-hadoop3/data/bankingdata/transaction.csv\")\n",
    "df2.show(5)\n",
    "print (df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/17 05:38:53 WARN SchemaService: Switching to query schema resolution\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df2.write \\\n",
    "  .format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .option(\"url\", \"bolt://192.168.18.211:7687\") \\\n",
    "  .option(\"database\", \"irfandi\") \\\n",
    "  .mode(\"append\") \\\n",
    "  .option(\"authentication.basic.username\", \"neo4j\") \\\n",
    "  .option(\"authentication.basic.password\", \"ddi123\") \\\n",
    "  .option(\"labels\", \":Transaction\") \\\n",
    "  .option(\"node.keys\", \"trx_id\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RELATIONSHIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HAS_ACCOUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----------+\n",
      "|        cif| account_no| open_date|\n",
      "+-----------+-----------+----------+\n",
      "|18346875234|  442222452|26-07-2017|\n",
      "|18752032720|13335322450|21-12-2021|\n",
      "|18752032720|13335322472|26-06-2013|\n",
      "|18806746420| 2237072704|19-03-2021|\n",
      "|18806746420|12015201770|16-07-2018|\n",
      "+-----------+-----------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "DataFrame[cif: string, account_no: string, open_date: string]\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.read.option(\"delimiter\", \";\").option(\"header\", \"true\").csv(\"/home/ddi/spark-3.3.0-bin-hadoop3/data/bankingdata/account.csv\")\n",
    "df1.show(5)\n",
    "print (df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.write \\\n",
    "  .format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .option(\"url\", \"bolt://192.168.18.211:7687\") \\\n",
    "  .option(\"database\", \"irfandi\") \\\n",
    "  .mode(\"append\") \\\n",
    "  .option(\"authentication.type\", \"basic\") \\\n",
    "  .option(\"authentication.basic.username\", \"neo4j\") \\\n",
    "  .option(\"authentication.basic.password\", \"ddi123\") \\\n",
    "  .option(\"relationship.save.strategy\", \"keys\") \\\n",
    "  .option(\"relationship\", \"HAS_ACCOUNT\") \\\n",
    "  .option(\"relationship.source.save.mode\", \"Overwrite\") \\\n",
    "  .option(\"relationship.source.labels\", \":Customer\") \\\n",
    "  .option(\"relationship.source.node.keys\", \"cif:cif\") \\\n",
    "  .option(\"relationship.target.save.mode\", \"Overwrite\") \\\n",
    "  .option(\"relationship.target.labels\", \":Account\") \\\n",
    "  .option(\"relationship.target.node.keys\", \"account_no:account_no\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SEND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------------+-----------+----------+-----------+\n",
      "|trx_id|  trx_date|account_from|     amount|account_to|trx_channel|\n",
      "+------+----------+------------+-----------+----------+-----------+\n",
      "|  3482|22/06/2021|   599280088| 25031037.0|1755637562|        SMS|\n",
      "|  4463|26/04/2021|    99677304|135827850.0|2266089634|        SMS|\n",
      "|  9002|17/05/2021|    99677304| 45697816.0| 186714864|        SMS|\n",
      "| 13021|26/08/2021|    18286974| 13895927.0| 349079568|        SMS|\n",
      "| 13118|23/08/2021|   411650844|   106963.0| 458800460|        SMS|\n",
      "+------+----------+------------+-----------+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "DataFrame[trx_id: string, trx_date: string, account_from: string, amount: string, account_to: string, trx_channel: string]\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.read.option(\"delimiter\", \";\").option(\"header\", \"true\").csv(\"/home/ddi/spark-3.3.0-bin-hadoop3/data/bankingdata/transaction.csv\")\n",
    "df2.show(5)\n",
    "print (df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/17 05:51:59 WARN SchemaService: Switching to query schema resolution\n",
      "22/10/17 05:51:59 WARN SchemaService: Switching to query schema resolution\n",
      "22/10/17 05:51:59 WARN SchemaService: Switching to query schema resolution\n",
      "22/10/17 05:51:59 WARN SchemaService: For the following exception\n",
      "org.neo4j.driver.exceptions.ClientException: Unable to convert scala.collection.immutable.Map$Map1 to Neo4j Value.\n",
      "\tat org.neo4j.driver.Values.value(Values.java:134)\n",
      "\tat org.neo4j.driver.internal.util.Extract.mapOfValues(Extract.java:203)\n",
      "\tat org.neo4j.driver.internal.AbstractQueryRunner.parameters(AbstractQueryRunner.java:69)\n",
      "\tat org.neo4j.driver.internal.AbstractQueryRunner.run(AbstractQueryRunner.java:43)\n",
      "\tat org.neo4j.spark.service.SchemaService.retrieveSchemaFromApoc(SchemaService.scala:68)\n",
      "\tat org.neo4j.spark.service.SchemaService.liftedTree2$1(SchemaService.scala:171)\n",
      "\tat org.neo4j.spark.service.SchemaService.structForRelationship(SchemaService.scala:155)\n",
      "\tat org.neo4j.spark.service.SchemaService.struct(SchemaService.scala:262)\n",
      "\tat org.neo4j.spark.DataSource.$anonfun$inferSchema$1(DataSource.scala:41)\n",
      "\tat org.neo4j.spark.DataSource.callSchemaService(DataSource.scala:29)\n",
      "\tat org.neo4j.spark.DataSource.inferSchema(DataSource.scala:41)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.getTableFromProvider(DataSourceV2Utils.scala:90)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.getTable$1(DataFrameWriter.scala:280)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:296)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df2.write \\\n",
    "  .format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .option(\"url\", \"bolt://192.168.18.211:7687\") \\\n",
    "  .option(\"database\", \"irfandi\") \\\n",
    "  .mode(\"append\") \\\n",
    "  .option(\"authentication.type\", \"basic\") \\\n",
    "  .option(\"authentication.basic.username\", \"neo4j\") \\\n",
    "  .option(\"authentication.basic.password\", \"ddi123\") \\\n",
    "  .option(\"relationship.save.strategy\", \"keys\") \\\n",
    "  .option(\"relationship\", \"SEND\") \\\n",
    "  .option(\"relationship.source.save.mode\", \"Overwrite\") \\\n",
    "  .option(\"relationship.source.labels\", \":Account\") \\\n",
    "  .option(\"relationship.source.node.keys\", \"account_from:account_no\") \\\n",
    "  .option(\"relationship.target.save.mode\", \"Overwrite\") \\\n",
    "  .option(\"relationship.target.labels\", \":Transaction\") \\\n",
    "  .option(\"relationship.target.node.keys\", \"trx_id:trx_id\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------------+-----------+----------+-----------+\n",
      "|trx_id|  trx_date|account_from|     amount|account_to|trx_channel|\n",
      "+------+----------+------------+-----------+----------+-----------+\n",
      "|  3482|22/06/2021|   599280088| 25031037.0|1755637562|        SMS|\n",
      "|  4463|26/04/2021|    99677304|135827850.0|2266089634|        SMS|\n",
      "|  9002|17/05/2021|    99677304| 45697816.0| 186714864|        SMS|\n",
      "| 13021|26/08/2021|    18286974| 13895927.0| 349079568|        SMS|\n",
      "| 13118|23/08/2021|   411650844|   106963.0| 458800460|        SMS|\n",
      "+------+----------+------------+-----------+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "DataFrame[trx_id: string, trx_date: string, account_from: string, amount: string, account_to: string, trx_channel: string]\n"
     ]
    }
   ],
   "source": [
    "df4= spark.read.option(\"delimiter\", \";\").option(\"header\", \"true\").csv(\"/home/ddi/spark-3.3.0-bin-hadoop3/data/bankingdata/transaction.csv\")\n",
    "df4.show(5)\n",
    "print (df4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RECEIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/17 06:05:21 WARN SchemaService: Switching to query schema resolution\n",
      "22/10/17 06:05:21 WARN SchemaService: Switching to query schema resolution\n",
      "22/10/17 06:05:21 WARN SchemaService: Switching to query schema resolution\n",
      "22/10/17 06:05:21 WARN SchemaService: For the following exception\n",
      "org.neo4j.driver.exceptions.ClientException: Unable to convert scala.collection.immutable.Map$Map1 to Neo4j Value.\n",
      "\tat org.neo4j.driver.Values.value(Values.java:134)\n",
      "\tat org.neo4j.driver.internal.util.Extract.mapOfValues(Extract.java:203)\n",
      "\tat org.neo4j.driver.internal.AbstractQueryRunner.parameters(AbstractQueryRunner.java:69)\n",
      "\tat org.neo4j.driver.internal.AbstractQueryRunner.run(AbstractQueryRunner.java:43)\n",
      "\tat org.neo4j.spark.service.SchemaService.retrieveSchemaFromApoc(SchemaService.scala:68)\n",
      "\tat org.neo4j.spark.service.SchemaService.liftedTree2$1(SchemaService.scala:171)\n",
      "\tat org.neo4j.spark.service.SchemaService.structForRelationship(SchemaService.scala:155)\n",
      "\tat org.neo4j.spark.service.SchemaService.struct(SchemaService.scala:262)\n",
      "\tat org.neo4j.spark.DataSource.$anonfun$inferSchema$1(DataSource.scala:41)\n",
      "\tat org.neo4j.spark.DataSource.callSchemaService(DataSource.scala:29)\n",
      "\tat org.neo4j.spark.DataSource.inferSchema(DataSource.scala:41)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.getTableFromProvider(DataSourceV2Utils.scala:90)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.getTable$1(DataFrameWriter.scala:280)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:296)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 37:>                                                         (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "df2.write \\\n",
    "  .format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .option(\"url\", \"bolt://192.168.18.211:7687\") \\\n",
    "  .option(\"database\", \"irfandi\") \\\n",
    "  .mode(\"append\") \\\n",
    "  .option(\"authentication.type\", \"basic\") \\\n",
    "  .option(\"authentication.basic.username\", \"neo4j\") \\\n",
    "  .option(\"authentication.basic.password\", \"ddi123\") \\\n",
    "  .option(\"relationship.save.strategy\", \"keys\") \\\n",
    "  .option(\"relationship\", \"RECEIVE\") \\\n",
    "  .option(\"relationship.source.save.mode\", \"Overwrite\") \\\n",
    "  .option(\"relationship.source.labels\", \":Transaction\") \\\n",
    "  .option(\"relationship.source.node.keys\", \"trx_id:trx_id\") \\\n",
    "  .option(\"relationship.target.save.mode\", \"Overwrite\") \\\n",
    "  .option(\"relationship.target.labels\", \":Account\") \\\n",
    "  .option(\"relationship.target.node.keys\", \"account_to:account_no\") \\\n",
    "  .save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
